# QuantizationOwnModel
I train and test a model on the MNIST dataset 
# 1. Post-Training Quantization (PTQ)
Post-training quantization is applied after a model has been trained. It converts the weights and activations of a trained model from floating point to lower precision (like INT8). 
# 2. Quantization-Aware Training (QAT)
Quantization-aware training is a technique where quantization is incorporated into the training process itself. This means that the effects of quantization are considered during the model optimization.
